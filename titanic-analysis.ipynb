{"cells":[{"cell_type":"markdown","source":["#Titanic Survivor Analysis"],"metadata":{}},{"cell_type":"markdown","source":["Let's read our dataset from s3, and explore provided variables"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import split, size\ntd = spark.read.csv('/mnt/ts/titanic.csv', header=True)\ntitanic_data = td.select(\n  td.pclass.cast('int'),\n  td.survived.cast('int'),\n  td.name,\n  td.sex,\n  td.age.cast('float'),\n  td.sibsp.cast('int'),\n  td.parch.cast('int'),\n  td.ticket,\n  td.fare.cast('float'),\n  td.cabin,\n  td.embarked,\n  td.boat,\n  td.body,\n  td.home_dest\n)\ntitanic_data.registerTempTable('titanic')\ntitanic_data.cache()\ndisplay(titanic_data)"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["Display the summary for provided data"],"metadata":{}},{"cell_type":"code","source":["display(titanic_data.describe())"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["##Handle missing values\n\nHandling missing values is a crucial part in feature engineering when working with data. There are many approaches to this issue, and we should defenitely try to avoid removing complete rows or columns from the dataset, due to the dataset size. We will try to impute missing values based on data distribution and prediction.\nIn this notebook we will go through the taken steps in handling missing values in Titanic Dataset. The following features with missing values were imputed in the dataset:\n* Age\n* Embarked\n* Fare  \nThe features _ticket, cabin, boat, body and home\\_destination_ will be unfortanutely removed from the dataset, since they contain many missing data fields, and imputing these values can give us false information based on the dataset size. So we are making a tough decision to work without these features."],"metadata":{}},{"cell_type":"markdown","source":["####Impute AGE\nFirst column that we will deal with is _age_. Titanic Dataset is a very well known Kaggle competition, and many people have provided very clever and interesting ideas to deal with the dataset. While dealing with missing values, we will use ideas shared by [Megan L. Risdal on Kaggle](https://www.kaggle.io/svf/924638/c05c7b2409e224c760cdfb527a8dcfc4/__results__.html), since this approach is quite interesting.\nSince the dataset is small in size, we will do the age imputation in R, using [Mice](https://cran.r-project.org/web/packages/mice/index.html) package. We will impute missing age values without some of the columns that give us less useful information. The code for the R script is available in a file _ageImputation.R_.<br>\nAfter running the code, we can compare the distribution of the age column before and after the imputation, just to make sure that we are not making any bad decisions:\n![Alt text](http://www.swiftpic.org/image.uploads/26-02-2017/original-8579452ad555b4ea57b9a155b38170ac.png)\n\nAfter making sure the distribution will be the same after imutation phase, we can impute the missing age values.<br>"],"metadata":{}},{"cell_type":"markdown","source":["### Impute Embarked\nBased on some general ideas on the columns of the Titanic Dataset, prior knowledge and provided information, we can assume that _fare_ and _passenger class_ information can help us retrieve the missing embarkment information. From the dataset, we can see that both passengers that are missing embarkment information paid  $80 for the travel and that they belong to the pclass=1.\nWe can plot the information from which place other travelers with the same pclass and fare values usually embarked. The code for plotting the chart is available in a script _embarkedImputation.R_.\n![Alt text](http://www.swiftpic.org/image.uploads/26-02-2017/original-501bfc4a379d8bef96604e36961ace11.png)\n<br>As we can see from the chart, other passengers that paid $80 (median fare) and belong to pclass=1 usually embarked from 'C'. Based on this information we can impute our missing values with for embarkment with 'C'."],"metadata":{}},{"cell_type":"markdown","source":["### Impute Fare\nThe final column with the missing value that we are going to deal with is _fare_ information. The passenger that is missing this information belongs to the third class, and embarked in 'S'. We can plot the median and distirbution of the _fare_ feature for the passengers that are from the same class and departed from the same place. (The R code is available in a script _fareImputation.R_) <br>\n![Alt text](http://www.swiftpic.org/image.uploads/26-02-2017/original-be2bcfd754f9b598aa2b80c840090a41.png) <br>\nIt seems reasonable based on the plot to replace the missing _fare_ value with median for their class and embarkment, which is $8.05."],"metadata":{}},{"cell_type":"markdown","source":["After dealing with the missing data in R, we are reading a final dataset, which contains all the imputed values."],"metadata":{}},{"cell_type":"code","source":["td = spark.read.csv('/mnt/ts/titanic_final.csv', header=True)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["After reading the dataset, we will retrieve state information from _home\\_dest_ column"],"metadata":{}},{"cell_type":"code","source":["titanic_data = td.select(\n  td.pclass.cast('int'),\n  td.survived.cast('int'),\n  td.name,\n  td.sex,\n  td.age.cast('float'),\n  td.sibsp.cast('int'),\n  td.parch.cast('int'),\n  td.ticket,\n  td.fare.cast('float'),\n  td.cabin,\n  td.embarked,\n  td.boat,\n  td.body,\n  td.home_dest,\n  split(split(td.home_dest, ('/'))[size(split(td.home_dest, ('/'))) -1], ',')[\n    size(split(split(td.home_dest, ('/'))[size(split(td.home_dest, ('/'))) -1], ',')) - 1\n  ].alias('state')\n)"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["Let's register a temporary table so we can use SQL for creating required reports."],"metadata":{}},{"cell_type":"code","source":["titanic_data.registerTempTable('titanic')"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["%sql\nselect pclass, count(1) as number_of_survivors\nfrom titanic\nwhere survived=1\ngroup by pclass"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["%sql\nselect sex, count(1) as number_of_survivors\nfrom titanic\nwhere survived=1\ngroup by sex"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["%sql\nselect age, count(1) as number_of_survivors\nfrom titanic\nwhere survived=1\ngroup by age\norder by age asc"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["%sql\nselect pclass, age, sex, count(*) as number_of_survivors\nfrom titanic\nwhere survived=1\ngroup by pclass, age, sex\norder by number_of_survivors desc"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["%sql\nselect state, count(*) as number_of_survivors\nfrom titanic\nwhere survived=1\ngroup by state\norder by number_of_survivors desc"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["### Feature Engineering\nLet's create some more features for our predictive model. We can see that each passenger has it's own title in a name, so let's retrieve titles for all our passengers, and count the occurences of each one of them."],"metadata":{}},{"cell_type":"code","source":["titanic_data = titanic_data.withColumn(\n  'title',\n  split(split(titanic_data.name, ',')[1], ' ')[1]\n)\ndisplay(titanic_data)"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["display(titanic_data.groupBy('title').count())"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":["We can see that some titles are more frequent than the other ones, so we will create a categorical feature with following possible values:\n* Mr.\n* Mrs.\n* Miss.\n* Master.\n* Other (for less frequent titles)"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql import functions as F\ntitanic = titanic_data.withColumn('ptitle', F.when(titanic_data.title=='Mr.', 'Mr.').when(titanic_data.title=='Mrs.', 'Mrs.').when(titanic_data.title=='Miss.', 'Miss.').when(titanic_data.title=='Master.', 'Master.').otherwise('Other'))\ntitanic = titanic.drop('title').withColumnRenamed('ptitle', 'title')"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["display(titanic)"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":["We will stop at this point with Feature Engineering. There are many great ideas for retrieving more features at the official [Kaggle page of the contest](https://www.kaggle.com/c/titanic), and I am defenitely recommending taking a peak there."],"metadata":{}},{"cell_type":"markdown","source":["# Predictive Modeling\nLet's start building our predictive models. <br><br><br>\nFirst step is to encode our categorical features and make feature vectors suitable for use in Spark ML pipelines and models."],"metadata":{}},{"cell_type":"code","source":["from pyspark.mllib.regression import LabeledPoint\nfrom pyspark.mllib.tree import DecisionTree\nfrom pyspark.ml.feature import OneHotEncoder, StringIndexer\n\n# display(encoded)\nfor x in [\"pclass\", \"embarked\", \"title\", \"sex\"]:\n  indexer = StringIndexer(inputCol=x, outputCol=x+\"Index\").fit(titanic)\n  indexed = indexer.transform(titanic)\n  encoder = OneHotEncoder(dropLast=False, inputCol=x+\"Index\", outputCol=x+\"Feature\")\n  titanic = encoder.transform(indexed)\ndisplay(titanic)"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":["We won't use all the available features in our models, we will focus on the most descriptive ones:\n* age\n* sibsp\n* fare\n* pclass\n* embarked\n* title\n* sex"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.feature import VectorAssembler\n\nassembler = VectorAssembler(\n    inputCols=[\"age\", \"sibsp\", \"parch\", \"fare\", \"pclassFeature\", \"embarkedFeature\", \"titleFeature\", \"sexFeature\"],\n    outputCol=\"features\")\n\ntransformed = assembler.transform(titanic)\ndisplay(transformed)"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":["##Random Forest"],"metadata":{}},{"cell_type":"markdown","source":["First model that we will test is Random Forest. The full documentation to the implementation of PySpark ML used is available in the official [PySpark documentation](https://spark.apache.org/docs/latest/ml-classification-regression.html#random-forest-classifier).<br>\nWe will split our dataset into training and test datasets. <br>\nNext step will be making a Pipeline that indexes all the labels in the label column and than trains the Random Forest model. <br>\nAfter building the model, we'll test it on our test dataset."],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml import Pipeline\nfrom pyspark.ml.classification import RandomForestClassifier\nfrom pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n\n# Let's index our labels, and fit on whole dataset to include all labels in index.\nlabelIndexer = StringIndexer(inputCol=\"survived\", outputCol=\"label\").fit(transformed)\n\n# Split the data into training and test sets (30% held out for testing)\n(trainingData, testData) = transformed.randomSplit([0.7, 0.3])\n\n# Train a RandomForest model.\nrf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=10)\n\n# Chain indexers and forest in a Pipeline\npipeline = Pipeline(stages=[labelIndexer, rf])\n\n# Run labelIndexer and train the model.\nmodel = pipeline.fit(trainingData)\n\n# Let's Make predictions on our test data.\npredictions = model.transform(testData)\ndisplay(predictions)"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"markdown","source":["For model evaluation we will use PySpark's BinaryClassificationEvaluator, and Area Under ROC Curve measure. The Area Under the ROC Curve (AUC) is a measure of how well a parameter can distinguish between two groups."],"metadata":{}},{"cell_type":"code","source":["# Let's evaluate our model using BinaryClassificationEvaluator. We will use Area Under ROC Curve measure to evaluate our model.\nevaluator = BinaryClassificationEvaluator(\n    labelCol=\"label\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\nareaUnderROC = evaluator.evaluate(predictions)\nprint(\"Area under ROC = %g\" % (areaUnderROC))"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"markdown","source":["We can see that our model behaves well with event the small number of trees (n=10)<br><br>\nLet's visualize our results:"],"metadata":{}},{"cell_type":"code","source":["results = predictions.select(['probability', 'label'])\nresults_collect = results.collect()\nresults_list = [(float(i[0][0]), 1.0-float(i[1])) for i in results_collect]"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"code","source":["from sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\n \nfpr = dict()\ntpr = dict()\nroc_auc = dict()\n \ny_test = [i[1] for i in results_list]\ny_score = [i[0] for i in results_list]\n \nfpr, tpr, _ = roc_curve(y_test, y_score)\nroc_auc = auc(fpr, tpr)\n\nfigure = plt.figure()\nplt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic example')\nplt.legend(loc=\"lower right\")\ndisplay(figure)"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"markdown","source":["Based on this plot, we can say that our results are not really bad, since our ROC curve (blue line) is far from the diagonal dashed line. <br><br>\nLet's see which features are the most important for our Random Forest model (we will use this information later on, in Feature Selection phase). <br>RandomForest.featureImportances computes, given a tree ensemble model, the importance of each feature.\n\nThis generalizes the idea of \"Gini\" importance to other losses, following the explanation of Gini importance from \"Random Forests\" documentation by Leo Breiman and Adele Cutler, and following the implementation from scikit-learn. The full explanation of the approach is documented in official [Spark documentation](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.classification.DecisionTreeClassificationModel.featureImportances)."],"metadata":{}},{"cell_type":"code","source":["model.stages[1].featureImportances"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"markdown","source":["## Decision Trees\nNext model that we are going to use for our classification task is [Decision Tree](https://spark.apache.org/docs/latest/ml-classification-regression.html#decision-trees).\nWe will take the similar steps as in Random Forest implementation, and we will use the same datasets for training and testing."],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml import Pipeline\nfrom pyspark.ml.classification import DecisionTreeClassifier\nfrom pyspark.ml.feature import StringIndexer, VectorIndexer\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\n\n# Intialize a Decision Tree Classifier\ndt = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\")\n\n# Make a Pipeline from Label Indexer and Decision Tree Model\npipeline = Pipeline(stages=[labelIndexer, dt])\n\n# Run Label Indexer and train the model\nmodel = pipeline.fit(trainingData)\n\n# Make predictions.\npredictions = model.transform(testData)"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"code","source":["evaluator = BinaryClassificationEvaluator(\n    labelCol=\"label\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\nareaUnderROC = evaluator.evaluate(predictions)\nprint(\"Area under ROC = %g\" % (areaUnderROC))"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"markdown","source":["Our Area under ROC curve for Decision Trees is not as good as with Random Forest alghorithm..."],"metadata":{}},{"cell_type":"code","source":["results1 = predictions.select(['probability', 'label'])\nresults_collect1 = results1.collect()\nresults_list1 = [(float(i[0][0]), 1.0-float(i[1])) for i in results_collect1]"],"metadata":{},"outputs":[],"execution_count":46},{"cell_type":"code","source":["from sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\n \nfpr = dict()\ntpr = dict()\nroc_auc = dict()\n \ny_test = [i[1] for i in results_list1]\ny_score = [i[0] for i in results_list1]\n \nfpr, tpr, _ = roc_curve(y_test, y_score)\nroc_auc = auc(fpr, tpr)\n\nfigure = plt.figure()\nplt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic example')\nplt.legend(loc=\"lower right\")\ndisplay(figure)"],"metadata":{},"outputs":[],"execution_count":47},{"cell_type":"markdown","source":["## Naive Bayes\nNext model that we are going to test is [Naive Bayes](https://spark.apache.org/docs/latest/ml-classification-regression.html#naive-bayes), probabilistic classifiers based on applying Bayes’ theorem with strong (naive) independence assumptions between the features.\n\nWe are going to use multinomial classification, although we are trying to predict one of the two possible outcomes."],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.classification import NaiveBayes\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\n\n# create the trainer and set its parameters\nnb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\")\n\npipeline = Pipeline(stages=[labelIndexer, nb])\n\n# Run Label Indexer and train the model\nmodel = pipeline.fit(trainingData)\n\n# Make predictions.\npredictions = model.transform(testData)"],"metadata":{},"outputs":[],"execution_count":49},{"cell_type":"code","source":["evaluator = BinaryClassificationEvaluator(\n    labelCol=\"survived\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\nareaUnderROC = evaluator.evaluate(predictions)\nprint(\"Area under ROC = %g\" % (areaUnderROC))"],"metadata":{},"outputs":[],"execution_count":50},{"cell_type":"markdown","source":["Naive Bayes is much more unprecise compared to Random Forest and Decison Trees"],"metadata":{}},{"cell_type":"code","source":["from sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\n \nfpr = dict()\ntpr = dict()\nroc_auc = dict()\n \ny_test = [i[1] for i in results_list]\ny_score = [i[0] for i in results_list]\n \nfpr, tpr, _ = roc_curve(y_test, y_score)\nroc_auc = auc(fpr, tpr)\n\nfigure = plt.figure()\nplt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic example')\nplt.legend(loc=\"lower right\")\ndisplay(figure)"],"metadata":{},"outputs":[],"execution_count":52},{"cell_type":"markdown","source":["## Logistic Regression"],"metadata":{}},{"cell_type":"markdown","source":["The final algorithm that we are going to try is [Logistic Regression](https://spark.apache.org/docs/latest/ml-classification-regression.html#logistic-regression). Logistic Regression is one of the most well-known algorithms that is used to predict one of the two possible outcomes."],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.classification import LogisticRegression\n\nlr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n\n# Fit the model\nlrModel = lr.fit(trainingData.select(trainingData.survived.alias('label'), trainingData.features))\n\n# Print the coefficients and intercept for logistic regression\nprint(\"Coefficients: \" + str(lrModel.coefficients))\nprint(\"Intercept: \" + str(lrModel.intercept))"],"metadata":{},"outputs":[],"execution_count":55},{"cell_type":"code","source":["predictions = lrModel.transform(testData.select(testData.survived, testData.features))\ndisplay(predictions)"],"metadata":{},"outputs":[],"execution_count":56},{"cell_type":"code","source":["evaluator = BinaryClassificationEvaluator(\n    labelCol=\"survived\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\nareaUnderROC = evaluator.evaluate(predictions)\nprint(\"Area under ROC = %g\" % (areaUnderROC))"],"metadata":{},"outputs":[],"execution_count":57},{"cell_type":"markdown","source":["Logistic Regression also shows less powerful results than Random Forest."],"metadata":{}},{"cell_type":"code","source":["display(predictions)"],"metadata":{},"outputs":[],"execution_count":59},{"cell_type":"code","source":["from pyspark.ml.classification import LogisticRegression\n\n# Extract the summary from the returned LogisticRegressionModel instance trained\n# in the earlier example\ntrainingSummary = lrModel.summary\n\n# Obtain the objective per iteration\nobjectiveHistory = trainingSummary.objectiveHistory\nprint(\"objectiveHistory:\")\nfor objective in objectiveHistory:\n    print(objective)"],"metadata":{},"outputs":[],"execution_count":60},{"cell_type":"code","source":["from pyspark.mllib.evaluation import BinaryClassificationMetrics as metric\nresults = predictions.select(['probability', 'survived'])\n \n## prepare score-label set\nresults_collect = results.collect()\nresults_list = [(float(i[0][0]), 1.0-float(i[1])) for i in results_collect]\nscoreAndLabels = sc.parallelize(results_list)\n \nmetrics = metric(scoreAndLabels)\nprint(\"The ROC score is (@numTrees=10): \", metrics.areaUnderROC)\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\n \nfpr = dict()\ntpr = dict()\nroc_auc = dict()\n \ny_test = [i[1] for i in results_list]\ny_score = [i[0] for i in results_list]\n \nfpr, tpr, _ = roc_curve(y_test, y_score)\nroc_auc = auc(fpr, tpr)\n\nfigure = plt.figure()\nplt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic example')\nplt.legend(loc=\"lower right\")\ndisplay(figure)"],"metadata":{},"outputs":[],"execution_count":61},{"cell_type":"markdown","source":["## Feature Selection"],"metadata":{}},{"cell_type":"markdown","source":["After comparing the results of four tested models, we can see that we are getting quite satisfying results with Random Forest, even for the small number of trees. Random Forest is performing much better than the other tested algorithms. In that manner, we will try to select only the most relevant features, and try to lower the dimensionality of the problem. <br>\nAfter evaluating the results of Random Forest, we have identified the importance of the used features:  \nSparseVector(17, {0: 0.0589, 1: 0.0454, 2: 0.028, 3: 0.0834, 4: 0.1145, 5: 0.0469, 6: 0.0257, 7: 0.019, 8: 0.0159, 9: 0.003, 10: 0.0059, 11: 0.0009, 12: 0.0524, 13: 0.0211, 14: 0.0021, 15: 0.2103, 16: 0.2664}).<br>  \nHere we are going to lower the dimensionality of the problem based on feature importance, and using Vector Slicer Feature Selecion. Let's test how our datasets behave with less features, and try to make more robust model while using smaller number of features."],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.feature import VectorSlicer\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.sql.types import Row\n\nslicer = VectorSlicer(inputCol=\"features\", outputCol=\"selectedFeatures\").setIndices([3, 4, 15, 16])\n\n# We are using the same datasets as for the other algorithms\noutput = slicer.transform(transformed)\notestData = slicer.transform(testData)\notrainData = slicer.transform(trainingData)\n\n# Let's make our model\nrf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"selectedFeatures\", numTrees=10)\n\n\n# Chain indexers and forest in a Pipeline\npipeline = Pipeline(stages=[labelIndexer, rf])\n\n# Train model.  This also runs the indexers.\nmodel = pipeline.fit(otrainData)\n\n# Make predictions.\npredictions = model.transform(otestData)\n\n# Select example rows to display.\n# display(predictions.select(\"prediction\", \"label\", \"features\"))\n\n# # Select (prediction, true label) and compute test error\nevaluator = BinaryClassificationEvaluator(\n    labelCol=\"label\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\nareaUnderROC = evaluator.evaluate(predictions)\nprint(\"Area under ROC = %g\" % (areaUnderROC))"],"metadata":{},"outputs":[],"execution_count":64},{"cell_type":"markdown","source":["We have selected 4 of the most predictive features, and the results are better than results of other algorthms. <br>\nLet's see will the performance grow significantly if we add one more feature."],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.feature import VectorSlicer\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.sql.types import Row\n\n# slicer = VectorSlicer(inputCol=\"features\", outputCol=\"selectedFeatures\").setIndices([3, 4, 15, 16])\nslicer = VectorSlicer(inputCol=\"features\", outputCol=\"selectedFeatures\").setIndices([0, 3, 4, 15, 16])\n\noutput = slicer.transform(transformed)\notestData = slicer.transform(testData)\notrainData = slicer.transform(trainingData)\n\n# display(output)\n# train, test = output.randomSplit([0.7, 0.3])\n# display(train)\nrf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"selectedFeatures\", numTrees=10)\n\n# Convert indexed labels back to original labels.\n# labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\",\n#                                labels=labelIndexer.labels)\n\n# Chain indexers and forest in a Pipeline\npipeline = Pipeline(stages=[labelIndexer, rf])\n\n# Train model.  This also runs the indexers.\nmodel = pipeline.fit(otrainData)\n\n# Make predictions.\npredictions = model.transform(otestData)\n\n# Select example rows to display.\n# display(predictions.select(\"prediction\", \"label\", \"features\"))\n\n# # Select (prediction, true label) and compute test error\nevaluator = BinaryClassificationEvaluator(\n    labelCol=\"label\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\nareaUnderROC = evaluator.evaluate(predictions)\nprint(\"Area under ROC = %g\" % (areaUnderROC))"],"metadata":{},"outputs":[],"execution_count":66},{"cell_type":"markdown","source":["Our results from Random Forest with reduced number of dimensions to 5 shows almost the same performance as our model trained on all the 17 features (Area under ROC with 17 features was 0.869552, and we have managed to achieve Area under ROC of 0.86754 while using only 5 of the most predictive features).\n### Final Conclusion\nIn this notebook we have analyzed Titanic Dataset, and modeled the factors that are related to a passenger surviving the crash. <br>\nIn exploratory phase we have used R to impute the missing values, and explain our variables. <br><br>\nFor the purpose of Predictive Modeling, we have tried various classification approaches available in Spark 2.1 ML Library. Random Forest algorithm showed best performance, so we have decided to keep it as our predictive model for this purpose. We have decided to use Area under ROC curve as our metric for model performance, which shows us the power of the model to make a distinction betweeen two groups.<br>\nAfter model exploration and model selection, we were dealing with model robustness. We have managed to represent predictive model with 5 dimensions instead of 17 that we have started with, and keep the model performance. <br>\nDeployed model is scalable and robust, and it can easily work with significantly larger datasets (although the dataset for this problem can not grow significantly).<br><br>\nSome possible improvements for the model might include creating more features from the original dataset, and trying to impute the other missing values or retrieve relevant information from these fields."],"metadata":{}}],"metadata":{"name":"titanic-analysis","notebookId":3529602503058087},"nbformat":4,"nbformat_minor":0}
